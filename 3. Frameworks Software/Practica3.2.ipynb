{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkHEHL9WLDeu"
   },
   "source": [
    "# Práctica 3.2. Red Neuronal en TensorFlow\n",
    "\n",
    "En esa práctica vamos a implementar una red neuronal muy sencilla en TensorFlow. Primero crearemos un conjunto de datos circular con SKLearn, y después crearemos la red capa por capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creación del dataset\n",
    "\n",
    "El siguiente código creará unos datos artificiales para entrenar nuestra red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "colab_type": "code",
    "id": "40hacwtuJh5V",
    "outputId": "077040b8-5571-4c78-9bba-c546fd23b892"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Creamos nuestros datos artificiales, donde buscaremos clasificar \n",
    "# dos anillos concéntricos de datos. \n",
    "X, Y = make_circles(n_samples=500, factor=0.5, noise=0.05)\n",
    "\n",
    "print(np.array(list(zip(X,Y)))[1:11], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos en X y en Y los datos. Pasemos a visualizarlos en un gráfico. Observar que tenemos 2 componentes (caracterísiticas, dimensiones) en la matrix X, y solo una dimensión en Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolución del mapa de predicción.\n",
    "res = 100 \n",
    "\n",
    "# Coordendadas del mapa de predicción.\n",
    "_x0 = np.linspace(-1.5, 1.5, res)\n",
    "_x1 = np.linspace(-1.5, 1.5, res)\n",
    "\n",
    "# Input con cada combo de coordenadas del mapa de predicción.\n",
    "_pX = np.array(np.meshgrid(_x0, _x1)).T.reshape(-1, 2)\n",
    "\n",
    "# Objeto vacio a 0.5 del mapa de predicción.\n",
    "_pY = np.zeros((res, res)) + 0.5\n",
    "\n",
    "# Visualización del mapa de predicción.\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pcolormesh(_x0, _x1, _pY, cmap=\"coolwarm\", vmin=0, vmax=1)\n",
    "\n",
    "# Visualización de la nube de datos.\n",
    "plt.scatter(X[Y == 0,0], X[Y == 0,1], c=\"skyblue\")\n",
    "plt.scatter(X[Y == 1,0], X[Y == 1,1], c=\"salmon\")\n",
    "\n",
    "plt.tick_params(labelbottom=False, labelleft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ec_TFbA5LAs-"
   },
   "source": [
    "## 2. Red neuronal en Tensorflow\n",
    "\n",
    "A continuación crearemos la red neuronal en tensorflow (TF). Recordemos que tensorflow es una API de programación a nivel 1 para redes neuronales. Básicamente, TF provee una librería de diferenciación automática, es decir, calcularán los gradientes locales de manera automática para optimizar cualquier arquitectura que diseñemos. Esto se consigue mediante la representación de la arquitectura con un grafo de computación (tal y como vimos en el tema 2.3 sobre backpropagation).\n",
    "\n",
    "Cada nodo del grafo puede representar una variable de entrada o una operación. Las conexiones entre los nodos son los tensores que fluyen a través del grafo. Esta estructura de grafo es ideal para calcular los gradientes y hacer diferenciación automática. Este grafo se declara explícitamente previamente a iniciar la sesión, como vimos en la práctica anterior.\n",
    "\n",
    "Antes de comenzar, importemos las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2017
    },
    "colab_type": "code",
    "id": "I4R-y55iJlAy",
    "outputId": "f7dbff5b-41f8-4056-96a4-e07f3610b5a1"
   },
   "outputs": [],
   "source": [
    "# Importar TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Para crear la visualización animada del entrenamiento\n",
    "from matplotlib import animation\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definiremos las variables que serán nuestros placeholders. Uno para la matriz X, otro para el vector Y, que serán de tipo float y tendrán las dimensiones iguales que los datos de entrada. Recordemos que los **placeholders** son los puntos de entradas de datos al grafo computacional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los puntos de entrada de la red, para la matriz X e Y.\n",
    "iX = tf.placeholder('float', shape=[None, X.shape[1]])\n",
    "iY = tf.placeholder('float', shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el learning rate, y configuraremos la arquitectura de red mediante un vector, nn, que nos indicará cuantas neuronas habrá en cada capa. En este caso, tenemos 4 capas, la de entrada (con 2 nodos, porque X tiene dos dimensiones), la de salida (con una neurona, porque Y solo tiene una dimensión, es clasificación binaria), y dos capas ocultas con 16 y 8 neuronas cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05           # learning rate\n",
    "nn = [2, 16, 8, 1]  # número de neuronas por capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a definir la red. Recordemos que una neurona artificial realiza una suma ponderada de sus entradas (según unos pesos), le suma un bias, y después pasa ese valor por una función de activación:\n",
    "\n",
    "$$ f(b_i + \\sum_{j=1}^{n_i} w_j x_j ) $$\n",
    "\n",
    "Estas operaciones se pueden representar de manera matricial, por tanto, en vez de tener unidades sueltas, tendremos matrices que representarán los pesos de cada capa.\n",
    "\n",
    "En TF, las **variables** serán los parámetros que serán optimizados en la red; en nuestro caso, los pesos W y los bias b. A continuación definimos la primera capa con tan solo:\n",
    "* W1, que serán los pesos que conecten la capa de entrada con la primera oculta. Será un tensor 2D de 2x16. Se inicializan de forma aleatoria siguiendo una normal.\n",
    "* b1, que será el bias para cada neurona en la primera capa oculta. Será un tensor 1D de 16 elementos. Se inicializan de forma aleatoria siguiendo una normal.\n",
    "* l1, que serán las activaciones de la primera capa oculta: multiplicamos la entrada (placeholder) iX por los pesos W1, le sumamos b1, y la hacemos pasar por la función de activación ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 1\n",
    "W1 = tf.Variable(tf.random_normal([nn[0], nn[1]]), name='Weights_1')\n",
    "b1 = tf.Variable(tf.random_normal([nn[1]]), name='bias_1')\n",
    "\n",
    "l1 = tf.nn.relu(tf.add(tf.matmul(iX, W1), b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos repetir el proceso para crear la segunda capa. En este caso W2 será un tensor 2D con los elementos necesarios para conectar la primera con la segunda capa, b2 será un tensor 1D con los bias, y l2 serán las activaciones. Esta vez, hay que multiplicar las activaciones de la capa anterior (en l1) por los pesos W2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 2\n",
    "W2 = tf.Variable(tf.random_normal([nn[1], nn[2]]), name='Weights_2')\n",
    "b2 = tf.Variable(tf.random_normal([nn[2]]), name='bias_2')\n",
    "\n",
    "l2 = tf.nn.relu(tf.add(tf.matmul(l1, W2), b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos con la capa 3. Esta capa es la de salida y tiene tan solo un nodo para predecir Y. Usaremos la función activación sigmoide, al tratarse de clasificación binaria, devolviendo tan solo un valor real entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 3\n",
    "W3 = tf.Variable(tf.random_normal([nn[2], nn[3]]), name='Weights_3')\n",
    "b3 = tf.Variable(tf.random_normal([nn[3]]), name='bias_3')\n",
    "\n",
    "# Vector de predicciones de Y.\n",
    "pY = tf.nn.sigmoid(tf.add(tf.matmul(l2, W3), b3))[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acabamos de definir el grafo computacional. Ahora pasemos a evaluarlo. Para ello, \n",
    "\n",
    "* definamos la función de pérdida, que será el MSE (error cuadrático medio) entre las predicciones Y y la entrada real Y.\n",
    "* indicamos el optimizador que queremos utilizar para minimizar el error. En el ejemplo, descenso por gradiente (esto corresponde al SGD). Le decimos que queremos minimizar los valores de la función de pérdida.\n",
    "* número de épocas, o pases que le daremos al conjunto de entrenamiento.\n",
    "* un vector donde guardaremos la evolución de la predicción en cada pase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de las predicciones.\n",
    "loss = tf.losses.mean_squared_error(pY, iY)\n",
    "\n",
    "# Definimos al optimizador de la red, para que minimice el error.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "n_steps = 1000 # Número de ciclos de entrenamiento.\n",
    "\n",
    "iPY = [] # Aquí guardaremos la evolución de las predicción, para la animación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, definimos la sesión y lanzamos a ejecutar la optimización sobre el grafo de computación que nos hemos definido. Primero inicializamos las variables, y después ejecutamos la sesión pasándole los datos de entrada. Cada 25 iteraciones recogeremos las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  \n",
    "  # Inicializamos todos los parámetros de la red, las matrices W y b.\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "  # Iteramos n pases de entrenamiento.\n",
    "  for step in range(n_steps):\n",
    "  \n",
    "    # Evaluamos al optimizador, a la función de coste y al tensor de salida pY. \n",
    "    # La evaluación del optimizer producirá el entrenamiento de la red.\n",
    "    _, _loss, _pY = sess.run([optimizer, loss, pY], feed_dict={ iX : X, iY : Y })\n",
    "    \n",
    "    # Cada 25 iteraciones, imprimimos métricas.\n",
    "    if step % 25 == 0: \n",
    "      \n",
    "      # Cálculo del accuracy.\n",
    "      acc = np.mean(np.round(_pY) == Y)\n",
    "      \n",
    "      # Impresión de métricas.\n",
    "      print('Step', step, '/', n_steps, '- Loss = ', _loss, '- Acc =', acc)\n",
    "      \n",
    "      # Obtenemos predicciones para cada punto de nuestro mapa de predicción _pX.\n",
    "      _pY = sess.run(pY, feed_dict={ iX : _pX }).reshape((res, res))\n",
    "\n",
    "      # Y lo guardamos para visualizar la animación.\n",
    "      iPY.append(_pY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, el siguiente código genera una animación de cómo ha evolucionado la red sobre los datos de entrada. Requiere tener instalado ffmpeg en el equipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- CÓDIGO ANIMACIÓN ----- #\n",
    "\n",
    "ims = []\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "print(\"--- Generando animación ---\")\n",
    "\n",
    "for fr in range(len(iPY)):\n",
    "  \n",
    "  im = plt.pcolormesh(_x0, _x1, iPY[fr], cmap=\"coolwarm\", animated=True)\n",
    "\n",
    "  # Visualización de la nube de datos.\n",
    "  plt.scatter(X[Y == 0,0], X[Y == 0,1], c=\"skyblue\")\n",
    "  plt.scatter(X[Y == 1,0], X[Y == 1,1], c=\"salmon\")\n",
    "\n",
    "  # plt.title(\"Resultado Clasificación\")\n",
    "  plt.tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "  ims.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejercicio\n",
    "\n",
    "Modifica la red con el mínimo número de nodos y capas ocultas para que la red sea capaz de aprender a clasificar los datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recomendación\n",
    "\n",
    "Se recomienda ver la fuente de esta práctica al completo en este vídeo: https://www.youtube.com/watch?v=qTNUbPkR2ao. La implementación se compara contra Keras y SKLearn."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "3 Maneras de Programar a una Red Neuronal.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
