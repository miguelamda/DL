{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3.7. Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos de estudiar dos ejemplos de clasificación a un ejemplo de regresión, intentando predecir un valor continuo en vez de una etiqueta discreta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. El Dataset de Precios de Casas de Boston\n",
    "\n",
    "Intentaremos predecir el precio medio de casas de un barrio de Boston (a partir de datos de los años 70). Para ello, usaremos alguna información asociada a algunas de las casas de ese barrio: tasa de crimen, impuestos locales, etc.\n",
    "\n",
    "A diferencia de los datasets vistos en los ejemplos anteriores, éste es realmente pequeño, apenas 506 anotaciones, que están divididas en 404/102 en entrenamiento/test. Además, cada una de las características de los datos de entrada usa una escala diferente, por ejemplo, algunos valores son proporciones (continuos en $[0,1]$), otros toman valores discretos entre 1 y 12, otros entre 0 y 100, etc.\n",
    "\n",
    "Podemos echar un vistazo a los datos (recuerda que es posible que la primera vez que ejecutes estas instrucciones se descargue el dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como se puede observar, cada dato tiene 13 características, que esperamos permitan obtener una relación funcional para predecir su precio, como por ejemplo:\n",
    "\n",
    "1. Ratio de crimen per cápita.\n",
    "2. Proporción de área residencial en lotes de 25.000 pies cuadrado.\n",
    "3. ...\n",
    "\n",
    "El objetivo es el valor medio de las casas, en miles de dólares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Los precios están, aproximadamente, entre 10.000\\\\$ y 50.000\\\\$ (precios de los años 70, claro).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparando los datos\n",
    "\n",
    "Sería problemático alimentar la red neuronal con los datos en bruto que proporciona el dataset, que hace uso de rangos tan diferentes. Si fuera así, la red debería aprender, además de la relación funcional, a adaptar automáticamente la heterogeneidad de los datos, lo que haría que el aprendizaje fuera más complicado. Por ello, una buena práctica de preprocesamiento consiste en hacer una normalización de cada característica: para cada una de ellas (una columna) se extrae la media de sus valores y se divide por su desviación estándar, de esta forma la nueva características estará centrada en 0 y con desviación estándar 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Observa que las medias y desviaciones se calculan solo en los datos de entrenamiento, ya que no debemos hacer uso de la información de test para nada, aunque la renormalización se realiza a los datos de ambos conjuntos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construyendo la red\n",
    "\n",
    "Debido a que hay pocas muestras para el entrenamiento, vamos a usar una red muy pequeña, con solo 2 capas ocultas, cada una de 64 unidades. En general, cuantos menos datos tengamos de entrenamiento, mayor será el sobreajuste, así que usar una red pequeña puede mitigar este efecto porque el número de parámetros en toda la red es más bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    # Puesto que vamos a necesitar instanciar\n",
    "    # el mismo modelo múltiples veces,\n",
    "    # usaremos una función para construirlo.\n",
    "    red = models.Sequential()\n",
    "    red.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    red.add(layers.Dense(64, activation='relu'))\n",
    "    red.add(layers.Dense(1))\n",
    "    red.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez hemos creado una función que construye el modelo, en vez de hacerlo directamente. La razón es porque en el procedimiento de validación que veremos más adelante tendremos que construir varios modelos similares.\n",
    "\n",
    "Observa que la red acaba en una capa con una sola unidad y sin activación (es lo que se llama una capa lineal). Esta configuración es habitual cuando se hace regresión escalar (de un solo valor), ya que las funciones de activación restringen el rango de la salida, por ejemplo, una `sigmoid` aprende a predecir valores en $[0,1]$, pero una activación lineal puede aprender cualquier valor.\n",
    "\n",
    "Observa también que se usa `mse` como función de pérdida, habitual en el caso de problemas de regresión. Para monitorizar el entrenamiento usamos `mae`, *Mean Absolute Error*, que es el valor absoluto de la diferencia entre las predicciones y los objetivos. Por ejemplo, un MAE de 0,5 en este problema significa que nuestras predicciones difieren en unos 500\\$ de media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-validación cruzada\n",
    "\n",
    "Para evaluar la red mientras ajustamos los hiperparámetros (como el número de epochs a usar), podríamos dividir simplemente el conjunto de entrenamiento en entrenamiento y validación, tal y como hicimos en ejemplos anteriores. Sin embargo, debido a que hay muy pocas muestras, el conjunto de validación acabaría siendo muy pequeño (unas 100 muestras), por lo que las métricas de validación pueden depender excesivamente de qué muestras concretas han terminado en cada conjunto, mostrando, posiblemente, una gran varianza.\n",
    "\n",
    "La mejor práctica en este tipo de situaciones es usar una **K-validación cruzada**, que consiste en dividir los datos disponibles en $K$ particiones (normalmente, entre 4 y 5), después instanciar $K$ modelos exactos, y entonces entrenar cada uno de los modelos usando $K-1$ de las particiones anteriores y evaluar sobre la restante. La métrica de evaluación será la media de las $K$ métricas obtenidas sobre las validaciones de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('procesando el fold #', i)\n",
    "    # Prepara los datos de validación: datos de la partición número k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepara los datos de entrenamiento: datos de todas las otras particiones\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Construir el modelo de Keras (ya compilado anteriormente)\n",
    "    red = build_model()\n",
    "    # Entrenar el modelo (en modo silencioso: verbose=0)\n",
    "    red.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    # Evaluar el modelo sobre los datos de validación\n",
    "    val_mse, val_mae = red.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como puedes observar, las posibles métricas de validación varían mucho (dependiendo de la ejecución, de 2,1 a 2,9, puede depender de la ejecución concreta), por lo que la media (2,39) es mucho más fiable que cada una de las otras valoraciones por separado... este es precisamente el valor de la K-validacion cruzada. Todavía estamos con un error significativo de unos 2.400\\\\$ en precios que están en un rango de entre 10.000\\\\$ y 50.000\\\\$, así que intentemos entrenar la red durante un poco más de tiempo: 200 epochs. Con el fin de guardar un registro de cómo de bien funciona el modelo en cada epoch, vamos a modificar el bucle de entrenamiento para almacenar el score de validación en cada epoch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Limpieza de memoria\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "all_mae_entrenamientos = []\n",
    "for i in range(k):\n",
    "    print('procesando el fold #', i)\n",
    "    # Prepara los datos de validación: datos de la partición número k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepara los datos de entrenamiento: datos de todas las otras particiones\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Construir el modelo de Keras (ya compilado anteriormente)\n",
    "    red = build_model()\n",
    "    # Evaluar el modelo sobre los datos de validación\n",
    "    entrenamiento = red.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_entrenamiento = entrenamiento.history['val_mean_absolute_error']\n",
    "    all_mae_entrenamientos.append(mae_entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos calcular la media de los valores MAE en cada epoch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_entrenamiento = [\n",
    "    np.mean([x[i] for x in all_mae_entrenamientos]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y representarlo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_entrenamiento) + 1), average_mae_entrenamiento)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validación MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Puede ser un poco difícil extraer conocimiento de esta gráfica debido a los problemas de escala que presenta y a la alta varianza, así que podemos hacer lo siguiente:\n",
    "\n",
    "* Omitir los primeros 10 puntos de los datos, que parecen mostrar una escala distinta al resto de la curva.\n",
    "* Reemplazar cada punto con una media exponencial de los puntos anteriores, con el fin de obtener una curva más suave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_mae_entrenamiento = smooth_curve(average_mae_entrenamiento[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_entrenamiento) + 1), smooth_mae_entrenamiento)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validación MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "De acuerdo con esta nueva gráfica, parece que el MAE de validación deja de mejorar tras 100-130 epochs, a partir de entonces empieza a haber sobreajuste.\n",
    "\n",
    "Tras haber ajustado otros parámetros del modelo (por ejemplo, el tamaño de las capas ocultas), podemos entrenar una versión final del modelo sobre todos los datos de entrenamiento y medir el rendimiento sobre los datos de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener un nuevo modelo compilado.\n",
    "red = build_model()\n",
    "# Entrenarlo sobre los datos de entrenamiento al completo\n",
    "red.fit(train_data, train_targets,\n",
    "          epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = red.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que sigue mostrando un error todavía muy alto, rondando los 2.500\\$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusiones\n",
    "\n",
    "* La regresión hace uso de funciones de pérdida distintas a la clasificación. La más común suele ser la *Mean Squared Error* (MSE).\n",
    "* Además, las métricas de evaluación también suelen ser distintas, por ejemplo, *Mean Absolute Error* (MAE). El término \"accuracy\" aquí no tiene sentido.\n",
    "* Cuando las características de los datos de entrada usan diferentes rangos, cada una de ellas ha de ser normalizada independientemente en la etapa de preprocesamiento.\n",
    "* Cuando tenemos pocos datos, usar K-validación cruzada puede ser un buen método para evaluar el modelo de forma más fiable.\n",
    "* Cuando hay pocos datos, es preferible usar redes pequeñas con pocas capas (normalmente, 1 o 2) con el fin de evitar un sobreajuste exagerado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
