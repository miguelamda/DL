{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Practica6_1_Modelos_de_texto_codificación_one_hot_e_inmersiones_de_palabras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguelamda/DL/blob/master/6.%20Modelos%20de%20Secuencias/Practica6.1.%20Modelos%20de%20texto%3A%20codificaci%C3%B3n%20one-hot%20e%20inmersiones%20de%20palabras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZbMwaaP97Er"
      },
      "source": [
        "# Práctica 6.1. Modelos de Texto\n",
        "\n",
        "En esta práctica vamos a trabajar con las nociones básicas de modelos de texto, o también conocidos como modelos del lenguaje). Primero, haremos una introducción a las inmersiones (embeddings) de palabras, y distintas formas de procesar texto en Keras. También haremos uso de Convoluciones 1D para detectar patrones en las secuencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BdJTef8Y97Ex",
        "outputId": "24c340ca-4264-4a5d-dd15-64acb33e2992"
      },
      "source": [
        "from tensorflow import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36N76AgD97Ez"
      },
      "source": [
        "## 1. Codificación one-hot de textos\n",
        "\n",
        "La codificación **One-hot**, que ya vimos en nuestros primeros ejemplos con Keras, es la forma más sencilla de transformar texto en vectores numéricos. \n",
        "\n",
        "Consiste, esencialmente, en asociar un índice entero único a cada palabra, y entonces codificar la palabra por medio de un vector de $N$ componentes ($N$ es el tamaño del vocabulario) en el que hay un 1 en la posición del índice de la palabra y 0s en el resto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Earm5lP397Ez"
      },
      "source": [
        "A continuación se muestra una implementación manual de codificación one-hot de texto (por palabra). Este código es solo ilustrativo, después veremos cómo hacer uso de Keras para que haga ésto por nosotros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8N82tu797E0",
        "outputId": "d6fac6c8-3170-4eae-e969-e8c8dca17e48"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Esto es nuestros datos iniciales; una entrada por \"sample\"\n",
        "# (en este ejemplo de juguete, un \"sample\" es tan solo una frase, pero\n",
        "# podría ser un documento entero)\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# Primero, construimos un indice para todos los tokens en los datos\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "    # Tokenizamos los samples mediante el método split.\n",
        "    # En la vida real, podríamos también eliminar la puntuación\n",
        "    # y los caracteres especiales de las muestras\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            # Asignar un índice único a cada palabra única\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            # Observa que no asigamos el índice 0 a nada\n",
        "\n",
        "# A continuación, vectorizamos nuestras muestras.\n",
        "# Sólo consideraremos las primeras palabras `max_length` en cada muestra.\n",
        "max_length = 10\n",
        "\n",
        "# Aquí es donde almacenamos los resultados\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1.\n",
        "        \n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9fAcM_R97E1"
      },
      "source": [
        "Keras proporciona funciones para realizar esta codificación directamente (incluso a nivel de carácter), y es preferible usarlas porque tienen en cuenta una casuística mayor respecto a los caracteres especiales, signos de puntuación, etc. Además, estas funciones permiten definir el tamaño del vocabulario considerando únicamente las $N$ palabras más comunes del dataset e ignorando el resto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ3JFn2m97E1"
      },
      "source": [
        "El siguiente ejemplo muestra cómo usar el Tokenizar de Keras para preprocesar un dataset de texto (strings):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HqqiPlV97E2",
        "outputId": "6f6362c2-11ed-45c3-ae2f-3043ed2e415d"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# Creamos un tokenizador, configurado para que sólo tome\n",
        "# en cuenta las 1000 palabras más comunes\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "# Esto construye el índice de palabras\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "# Esto convierte las cadenas en listas de índices enteros.\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "# También se pueden obtener directamente las representaciones binarias one-hot.\n",
        "# Ten en cuenta que se admiten otros modos de vectorización además de la codificación one-hot.\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "# Esto es como puedes recuperar el índice de la palabra que fue calculada\n",
        "word_index = tokenizer.word_index\n",
        "print('Hay %s tokens distintos.' % len(word_index))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hay 9 tokens distintos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t2QgP8Z97E2"
      },
      "source": [
        "\n",
        "Hay variantes similares para reducir el tamaño del vocabulario por medio de funciones hash, pero no las veremos aquí."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM3eFZkr97E3"
      },
      "source": [
        "## 2. Inmersiones de palabras\n",
        "Otra forma habitual de asociar vectores a palabras es por medio de lo que se conocen como **inmersiones de palabras** (word embedding) o **vectorización densa**. Mientras que los vectores que se obtienen por la codificación one-hot son binarios, muy dispersos (casi todo son 0s) y de dimensión muy alta (el tamaño del vocabulario, que es fácil que alcance las decenas de miles), las inmersiones son de dimensión baja (entre 100 y 1000, aproximadamente), y con vectores de punto flotante. Además, y a difrencia del one-hot, las inmersiones se aprenden a partir de datos.\n",
        "\n",
        "Hay dos formas de obtener estas inmersiones:\n",
        "\n",
        "1. Aprenderlas junto con la tarea en la que se van a utilizar (por ejemplo, clasificación de documentos o análisis de sentimientos). Se sigue un proceso similar al que hemos visto al aprender los pesos de redes neuronales.\n",
        "2. Usando Transfer Learning, reutilizando un modelo de inmersión que ha sido pre-entrenado usando una tarea diferente de ML.\n",
        "\n",
        "Veremos con detalle estos dos métodos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT7jM6fP97E3"
      },
      "source": [
        "### 2.1. Aprendizaje de inmersiones\n",
        "\n",
        "Como el objetivo es que la información semántica de las palabras se refleje en el vector asociado, la asignación aleatoria de vectores a palabras, aunque posible, no tiene ninguna utilidad.\n",
        "\n",
        "El objetivo que se persigue cuando se aprende una inmersión es que las relaciones semánticas y gramaticales existentes entre las palabras del dataset se reflejen en propiedades geométricas de los vectores. Así, dos palabras que tengan un uso/significado similar deberían asociarse a vectores cercanos.\n",
        "\n",
        "Incluso, más allá de la simple distancia, sería deseable que las direcciones vectoriales también tengan algún significado en la inmersión. Cuando hacemos inmersiones de palabras de datasets del mundo real, resulta que las direcciones se asocian a conceptos como “plural” o “género”. Por ejemplo, si sumamos el vector “femenino” al vector asociado a la palabra “rey”, deberíamos obtener el vector asociado a la palabra “reina”, o si le sumamos el vector “plural”, obtenemos el vector asociado a la palabra “reyes”.\n",
        "\n",
        "<img src=\"https://2.bp.blogspot.com/-yL_425HS2ck/WEDZLk5cq0I/AAAAAAAABcI/kwy4F4Cmfi4jyG_InIiYu6F7y2-BKTXWQCLcB/s640/embedding-mnist.gif\" />\n",
        "\n",
        "Estas características no son extrapolables a todos los problemas, y dependen de la tarea concreta que queremos resolver, así pues tiene sentido entrenar la inmersión dependiendo del tipo de tarea a resolver. Para ello Keras introduce un tipo de capa llamada *embedding*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXsRU5yj97E3"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "# Las capas de tipo Embedding toman al menos dos argumentos:\n",
        "# el número de posibles tokens, y la dimensión de inmersión.\n",
        "embedding_layer = Embedding(1000, 64)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzzxhTbd97E4"
      },
      "source": [
        "Una capa *embedding* actúa como un diccionario que asocia índices enteros (palabras) a vectores densos. Formalmente, toma tensores 2D de enteros de tamaño `(samples, sequence_length)`, donde una entrada es una secuencia de enteros, y devuelve un tensor 3D de punto flotante de tamaño `(samples, sequence_length, embedding_dimensionality)`. Este tensor 3D puede ser procesado por las capas adecuadas (por ejemplo, una capa RNN o 1D-convolucional, como veremos más adelante).\n",
        "\n",
        "Cuando se instancia una capa de embedding, sus pesos (diccionario interno de vectores) se inician aleatoriamente, como en cualquier otra capa. Durante el entrenamiento, estos vectores se ajustan gradualmente por medio de la retropropagación, convirtiendo el espacio de vectores en algo que el modelo puede aprovechar. Una vez entrenado, el espacio de inmersión muestra una esctructura que refleja su especialización para el problema específico para el que ha sido entrenado.\n",
        "\n",
        "Vamos a aplicar esta idea al problema de análisis de sentimientos de las opiniones de IMDB que vimos al principio del curso. Vamos a restringirnos solamente a las 10.000 palabras más frecuentes que aparecen en las opiniones, y de cada opinión nos vamos a fijar únicamente en las primeras 20 palabras. La inmersión la haremos sobre un espacio de dimensión muy pequeña, 8D (cada palabra se convierte en un vector denso de 8 posiciones), posteriormente aplanamos esta información, y se la pasamos a una capa densa simple unitaria que realizará la clasificación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY3dz3lU97E4"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import utils\n",
        "\n",
        "# Número máximo de palabras a considerar como features\n",
        "max_features = 10000\n",
        "\n",
        "# Cargar los datos como listas de enteros.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Truncaremos los textos que tengan más de `maxlen` palabras, o las rellenaremos\n",
        "# con 0s hasta alcanzar este tamaño\n",
        "max_len = 20\n",
        "\n",
        "# x_train y x_test son listas de listas de enteros. Lo siguiente convierte\n",
        "# nuestras listas de enteros en un tensor 2D de enteros `(samples, maxlen)``\n",
        "x_train = utils.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = utils.pad_sequences(x_test, maxlen=max_len)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_TThvGn97E4"
      },
      "source": [
        "Creamos el modelo con la capa de inmersión, y realizamos el entrenamiento con los datos leídos:\n",
        "![](https://github.com/miguelamda/DL/blob/master/6.%20Redes%20Recurrentes/model1_plot.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtza5fBE97E5",
        "outputId": "42b6e9ba-3787-4be1-e9e5-c30b2273448d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "\n",
        "model = Sequential()\n",
        "# Especificamos la longitud máxima de entrada a nuestra capa de embedding\n",
        "# para que luego podamos aplanar las entradas inmersas\n",
        "model.add(Embedding(10000, 8, input_length=max_len))\n",
        "# Después de la capa de embedding, \n",
        "# nuestras activaciones tienen forma `(samples, max_len, 8)`.\n",
        "\n",
        "# Aplanamos el tensor 3D de los embeddings\n",
        "# en un tensor de forma 2D `(muestras, max_len * 8)``\n",
        "model.add(Flatten())\n",
        "\n",
        "# Añadimos el clasificador en la parte superior\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "#plot_model(model, to_file='model1_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 160)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 5s 4ms/step - loss: 0.6829 - acc: 0.5709 - val_loss: 0.6568 - val_acc: 0.6610\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5761 - acc: 0.7397 - val_loss: 0.5462 - val_acc: 0.7236\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4752 - acc: 0.7835 - val_loss: 0.5082 - val_acc: 0.7402\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4278 - acc: 0.8080 - val_loss: 0.4987 - val_acc: 0.7498\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3970 - acc: 0.8204 - val_loss: 0.4976 - val_acc: 0.7520\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3726 - acc: 0.8348 - val_loss: 0.4988 - val_acc: 0.7532\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3508 - acc: 0.8490 - val_loss: 0.5031 - val_acc: 0.7502\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3302 - acc: 0.8597 - val_loss: 0.5100 - val_acc: 0.7542\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3100 - acc: 0.8704 - val_loss: 0.5166 - val_acc: 0.7546\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2906 - acc: 0.8819 - val_loss: 0.5258 - val_acc: 0.7516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkd-T4ch97E5"
      },
      "source": [
        "Se consigue una precisión de validación que ronda el ~75%, lo que no está nada mal teniendo en cuenta que solo usamos 20 palabras de cada opinión (comprueba que si subes a 50, la validación alcanza el 80%). \n",
        "\n",
        "El modelo que hemos creado no es muy ajustado, ya que al poner una sola capa densa tras aplanar los datos hace que la posición que ocupa cada palabra no tenga ningún peso en el entrenamiento, y al perder la estructura de las frases se pierde la semántica y solo considera la aparición de ellas (por ejemplo, estas frases, que en inglés significan cosas muy distintas, las considera prácticamente iguales _“this movie is shit”_ y _“this movie is the shit”_). Para evitar este problema sería necesario usar una estructura de red que sí considerase la distribución espacial de las palabras, tal y como las convolucionales 2D hacen con las imágenes. Más adelante veremos cómo aplicar a este problema capas recurrentes o 1D convolucionales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9q8W9vR97E5"
      },
      "source": [
        "### 2.2. Uso de inmersiones preentrenadas\n",
        "\n",
        "Para obtener buenos resultados en inmersiones es necesario disponer de muchos datos, pero cuando los datos que tenemos no son suficientes podemos reusar el entrenamiento realizado en otras tareas, tal y como vimos en módulos anteriores. \n",
        "\n",
        "Al igual que las primeras capas convolucionales de una CNN preentrenada en clases genéricas reflejan relaciones generales útiles acerca de tareas de visión para muchas otras tareas, cuando tratamos con textos, si la tarea en la que se ha entrenado la inmersión no es muy subjetiva, las relaciones geométricas de los vectores de dicha inmersión reflejarán relaciones semánticas generales entre las palabras, que muy probablemente puedan ser útiles para otras tareas que usan el mismo vocabulario.\n",
        "\n",
        "Este tipo de inmersiones de palabras normalmente se contruyen a partir de estadísticas de ocurrencias de palabras (observaciones acerca de qué palabras co-ocurren en frases y documentos), usando diversas técnicas, algunas relacionadas con redes neuronales y otras que no. \n",
        "\n",
        "La idea de una inmersión de palabras densa y de baja dimensión, calculada de forma no supervisada fue inicialmente explorada por Bengio et al. a principios de los 2000, pero su uso real explotó a raíz del esquema de inmersión [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), desarrollado por Mikolov en Google en 2013, que captura propiedades semánticas tales como el género, número, sinonomia, etc.\n",
        "\n",
        "Hay varias bases de datos de inmersiones preentrenadas que se pueden descargar y usar desde las capas de inmersiones de Keras. Word2Vec es una de ellas, y [GloVe (“Global Vectors for Word Representation”)](https://nlp.stanford.edu/projects/glove/) es otra, desarrollada por investigadores de Stanford en 2014, que hace uso de factorización de matrices sobre estadísticas de co-ocurrencias a partir de millones de tokens extraídos de fuentes como la Wikipedia o Common Crawl. Puedes visualizar la inmersión en un espacio tri-dimensional en esta web https://projector.tensorflow.org.\n",
        "\n",
        "Vamos a mostrar cómo usar GloVe en Keras, pero el procedimiento que seguiremos es extrapolable a Word2Vec y otras inmersiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S-WxZ3n97E6"
      },
      "source": [
        "### 2.3. Del texto en bruto a las inmersiones de palabras\n",
        "\n",
        "Vamos a usar un modelo similar al anterior: \n",
        "* inmersión + \n",
        "* aplanamiento + \n",
        "* capa densa, \n",
        "\n",
        "pero esta vez usaremos una inmersión preentrenada, y en vez de usar el paquete de IMDB pre-tokenizado que proporciona Keras, haremos uso del dataset IMDB con el texto en bruto al completo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2QV3dQE97E7"
      },
      "source": [
        "#### Descarga de IMDB como texto\n",
        "\n",
        "El conjunto de datos sobre el que trabajaremos se puede descargar de http://ai.stanford.edu/~amaas/data/sentiment.\n",
        "\n",
        "En primer lugar, vamos a recolectar las opiniones individuales de entrenamiento como una lista de cadenas, una cadena para cada opinión, junto con las etiquetas asociadas (pos / neg) en una lista labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9DjdDri_QLJ",
        "outputId": "93015bd2-73f4-46b3-c093-1a10354c13a3"
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-12 15:59:02--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  34.5MB/s    in 2.3s    \n",
            "\n",
            "2023-01-12 15:59:05 (34.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bot8ORc97E7"
      },
      "source": [
        "import os\n",
        "\n",
        "# indica aquí la ruta a la carpeta descargada\n",
        "imdb_dir = './aclImdb/'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Vs-BV-97E7"
      },
      "source": [
        "#### Tokenizar los datos\n",
        "\n",
        "A continuación vamos a vectorizar los textos que hemos recolectado, y los dividiremos en conjunto de entrenamiento y de validación, de forma similar a como hemos hecho en ejemplos anteriores.\n",
        "\n",
        "Para poner a prueba lo buenas que son las inmersiones pre-entrenadas, vamos a trabajar sobre un conjunto de entrenamiento especialmente pequeño, y consideraremos únicamente 2000 muestras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRBW5uwy97E8",
        "outputId": "82a803ad-d8c8-404f-cca8-8935a24058f0"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # Recortaremos las críticas después de 100 palabras\n",
        "training_samples = 2000  # Estaremos entrenando con 2000 muestras\n",
        "validation_samples = 10000  # Estaremos validando con 10.000 muestras\n",
        "max_words = 10000  # Sólo consideraremos las 10.000 palabras más importantes del conjunto de datos\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Encontrados %s tokens únicos.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Dividir los datos en un conjunto de entrenamiento y un conjunto de validación\n",
        "# Pero primero, barajar los datos, ya que empezamos a partir de los datos\n",
        "# donde se ordenan las muestras (todas negativas primero, luego todas positivas).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "xl_train = data[:training_samples]\n",
        "yl_train = labels[:training_samples]\n",
        "xl_val = data[training_samples: training_samples + validation_samples]\n",
        "yl_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encontrados 88582 tokens únicos.\n",
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98Af2YpD97E8"
      },
      "source": [
        "#### Descargando GloVe\n",
        "\n",
        "Vamos a utilizar la inmersión preentrenada GloVe que ha sido creada a partir de la Wikipedia en inglés en 2014 https://nlp.stanford.edu/projects/glove/. Es un fichero de 822MB llamado `glove.6B.zip`, y contiene una imersión vectorial de 100D de 400.000 palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daWYMElL97E9"
      },
      "source": [
        "#### Preprocesando la inmersión\n",
        "\n",
        "El fichero anterior es un `txt` que debe ser procesado para construir la asociación entre palabras (cadenas) y vectores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otlma854_m-_",
        "outputId": "0202bf14-58cb-477d-85ec-ec4507c89c7c"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d glove.6B"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-12 16:09:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-01-12 16:09:30--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-01-12 16:09:30--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.98MB/s    in 2m 41s  \n",
            "\n",
            "2023-01-12 16:12:11 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B/glove.6B.50d.txt  \n",
            "  inflating: glove.6B/glove.6B.100d.txt  \n",
            "  inflating: glove.6B/glove.6B.200d.txt  \n",
            "  inflating: glove.6B/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55I__tHa97E9",
        "outputId": "993ae9f0-7f61-490a-90dc-bbd034ed0298"
      },
      "source": [
        "# indica aquí la ruta a la carpeta descargada\n",
        "glove_dir = 'glove.6B'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Encontrados %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encontrados 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpd09gKr97E9"
      },
      "source": [
        "A continuación, hemos de construir la matriz que cargaremos en la capa de inmersión. Debe tener tamaño `(max_words, embedding_dim)`, donde cada entrada $i$ contiene el vector de tamaño `embedding_dim` para la palabra $i$-ésima en el índice (construido durante la tokenización). El índice 0 no se usa para ninguna palabra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_LmMUuH97E9"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Las palabras no encontradas en el índice de embedding será todo cero.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1SNiWk897E_"
      },
      "source": [
        "#### Definiendo el modelo\n",
        "\n",
        "Usaremos la misma arquitectura que en el modelo anterior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwVS065h97E_",
        "outputId": "3b869c0b-0fb3-40d5-8e75-dd23f4665d33"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnI08muo97E_"
      },
      "source": [
        "#### Cargando la inmersión GloVe en el modelo\n",
        "\n",
        "La capa de inmersión tiene una matriz simple, 2D, de coma flotante, en la que cada entrada $i$ es el vector asociado a la $i$-ésima palabra. Basta cargar la matriz GloVe que hemos preparado antes en la capa de inmersión, que es la primera del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N1RH_3B97E_"
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z96b_c4P97FA"
      },
      "source": [
        "Además, hemos de congelar los pesos de esta capa, para que el entrenamiento posterior no modifique los vectores precargados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7j2OhJE97FA"
      },
      "source": [
        "#### Entrenamiento y Evaluación\n",
        "\n",
        "Compilemos y entrenemos el modelo (y lo grabamos):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzMu1IOR97FA",
        "outputId": "6e7eec42-34a8-4e81-bb92-bae20132e185"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(xl_train, yl_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(xl_val, yl_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 3s 33ms/step - loss: 0.7996 - acc: 0.5280 - val_loss: 0.6878 - val_acc: 0.5078\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.6898 - acc: 0.5830 - val_loss: 0.6840 - val_acc: 0.5297\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.6323 - acc: 0.6490 - val_loss: 0.8262 - val_acc: 0.5128\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.5352 - acc: 0.7320 - val_loss: 0.6942 - val_acc: 0.5904\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.4466 - acc: 0.7915 - val_loss: 0.7994 - val_acc: 0.5628\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.3682 - acc: 0.8420 - val_loss: 0.7011 - val_acc: 0.6235\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.2874 - acc: 0.8845 - val_loss: 0.9318 - val_acc: 0.5624\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.2165 - acc: 0.9225 - val_loss: 0.9056 - val_acc: 0.6179\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.1723 - acc: 0.9405 - val_loss: 0.8253 - val_acc: 0.6193\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.1310 - acc: 0.9550 - val_loss: 0.7807 - val_acc: 0.6494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29I0MS4_97FA"
      },
      "source": [
        "Y mostramos su rendimiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "-NLPa50_97FB",
        "outputId": "8af98055-9384-417b-c520-3281e6f5b5a8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaJ8Y2F_97FB"
      },
      "source": [
        "El modelo se sobreajusta muy rápido, algo que no es extraño teniendo en cuenta el tamaño reducido del conjunto de entrenamiento. Consecuentemente, la validación tiene una alta varianza y apenas supera el 63% en el mejor de los casos.\n",
        "\n",
        "Observa que, debido a la selección aleatoria del pequeño conjunto de entrenamiento, los resultados pueden variar mucho si se vuelve a ejecutar el procedimiento.\n",
        "\n",
        "También podemos intentar realizar el entrenamiento sin haber precargado una inmersión y sin congelar la capa de inmersión. Es decir, aprendiendo la inmersión específica de la tarea que estamos llevando a cabo. Esta opción, como comentamos, suele ser mejor si se dispone de los recursos computacionales necesarios y de muchos datos, pero nosotros lo entrenamos únicamente con 2000 muestras."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Usando Convolución 1D\n",
        "\n",
        "En Keras, se puede usar una convnet 1D a través de la capa `Conv1D`, que tiene una interfaz muy similar a `Conv2D`. Toma como entrada tensores 3D con forma `(samples, time, features)` y también devuelve tensores 3D de forma similar. La ventana de convolución es una ventana 1D en el eje temporal (es decir, eje 1, *time*, en el tensor de entrada).\n",
        "\n",
        "Las convnets 1D están estructuradas de la misma manera que sus versiones 2D que ya hemos utilizado en módulo 5: consisten en una pila de capas `Conv1D` y `MaxPooling1D`, terminando eventualmente en una capa de `globalPooling` o una capa `Flatten`, convirtiendo las salidas 3D en salidas 2D, permitiendo añadir una o más capas `Dense` al modelo, para hacer clasificación o regresión.\n",
        "\n",
        "Una diferencia, sin embargo, es el hecho de que podemos permitirnos el lujo de usar ventanas de convolución más grandes con convnets 1D. De hecho, con una capa de convolución 2D, una ventana de convolución 3x3 contiene vectores con `3*3 = 9` características, pero con una capa de convolución 1D, una ventana de convolución de tamaño 3 sólo contendría vectores con 3 características. De esta manera podemos permitirnos fácilmente ventanas de convolución 1D de tamaño 7 o 9.\n",
        "\n",
        "Este es nuestro ejemplo de convneto 1D para el conjunto de datos IMDB:"
      ],
      "metadata": {
        "id": "_MEvVqRFkFQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "modelConv1D = Sequential()\n",
        "# Aquí podemos ver que podemos crear una capa de embedding con los pesos\n",
        "# pre-entrenados directamente en la creación de la capa\n",
        "modelConv1D.add(layers.Embedding(max_words, \n",
        "                                 embedding_dim, \n",
        "                                 input_length=maxlen,\n",
        "                                 embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                                 trainable=False))\n",
        "modelConv1D.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "modelConv1D.add(layers.MaxPooling1D(5))\n",
        "modelConv1D.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "modelConv1D.add(layers.GlobalMaxPooling1D())\n",
        "modelConv1D.add(layers.Dense(1))\n",
        "\n",
        "modelConv1D.summary()\n",
        "\n",
        "modelConv1D.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "                    loss='binary_crossentropy',\n",
        "                   metrics=['acc'])\n",
        "history = modelConv1D.fit(xl_train, yl_train,\n",
        "                          epochs=10,\n",
        "                          batch_size=32,\n",
        "                         validation_data=(xl_val, yl_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1uLYefbkO4J",
        "outputId": "6ba70bd6-7cd1-40b4-ddab-faba1c6c9ecf"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 94, 32)            22432     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 18, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 12, 32)            7200      \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 32)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,029,665\n",
            "Trainable params: 29,665\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 10s 30ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 7.8744 - acc: 0.4895 - val_loss: 7.8143 - val_acc: 0.4934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veremos, cuando hagamos uso de redes recurrentes como la LSTM, que el tiempo de entrenamiento es muy alto. Sin embargo, el tiempo de ejecución de una Conv1D es más rápido, tanto en la CPU como en la GPU (aunque la velocidad exacta variará mucho dependiendo de la configuración)."
      ],
      "metadata": {
        "id": "0k3_jO_DnC8t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "7x9RntOT97FB",
        "outputId": "fef790be-eb48-4c9c-e1a6-2214d51c0a71"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9vQIMw797FC"
      },
      "source": [
        "Apenas conseguimos un 60% de accuracy en validación, un valor muy bajo que nos indica que en efecto trabajar con tan pocos ejemplos de entrenamiento lo convierte en una tarea dura.\n",
        "\n",
        "Vamos a intentarlo de nuevo con más ejemplos, usemos el dataset IMDB que nos provee Keras, ya pre-tokenizado. Pongamos una capa de Embedding vacía."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelConv1D_2 = Sequential()\n",
        "modelConv1D_2.add(layers.Embedding(max_features, output_dim=128, input_length=max_len ))\n",
        "modelConv1D_2.add(layers.Conv1D(32, 11, activation='relu'))\n",
        "modelConv1D_2.add(layers.Conv1D(16, 7, activation='relu'))\n",
        "modelConv1D_2.add(layers.GlobalMaxPooling1D())\n",
        "modelConv1D_2.add(layers.Dense(1))\n",
        "\n",
        "modelConv1D_2.summary()\n",
        "\n",
        "modelConv1D_2.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = modelConv1D_2.fit(x_train, y_train,\n",
        "                            epochs=10,\n",
        "                            batch_size=128,\n",
        "                            validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TrnHi8L7xzK",
        "outputId": "97e3ebef-f3dc-4f92-ea88-2679a77dbe72"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 20, 128)           1280000   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 10, 32)            45088     \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 4, 16)             3600      \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 16)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,328,705\n",
            "Trainable params: 1,328,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.4125 - acc: 0.4985 - val_loss: 0.7273 - val_acc: 0.5072\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.6718 - acc: 0.5806 - val_loss: 0.6537 - val_acc: 0.6208\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 1s 9ms/step - loss: 0.6127 - acc: 0.6814 - val_loss: 0.6106 - val_acc: 0.6662\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.5518 - acc: 0.7333 - val_loss: 0.5886 - val_acc: 0.6930\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.5066 - acc: 0.7635 - val_loss: 0.5899 - val_acc: 0.7088\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4718 - acc: 0.7875 - val_loss: 0.6007 - val_acc: 0.7184\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4428 - acc: 0.8030 - val_loss: 0.6357 - val_acc: 0.7218\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.4196 - acc: 0.8164 - val_loss: 0.6709 - val_acc: 0.7254\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.4011 - acc: 0.8296 - val_loss: 0.6887 - val_acc: 0.7262\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.3781 - acc: 0.8406 - val_loss: 0.7187 - val_acc: 0.7290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora mucho mejor. Conseguimos un rendimiento mucho mejor, similar al obtenido al comienzo de la práctica. Jugando con la arquitectura y los hiperparámetros podríamos conseguir una mejora sustancial."
      ],
      "metadata": {
        "id": "XxxFan_iFU-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Usando vectorización de texto y carga de datasets de texto\n",
        "\n",
        "Acabamos de ver una forma de tokenizar el texto un poco \"manual\", transformando el texto a vectores de enteros. El entrenamiento del modelo se hace directamente sobre los datos transformados. Usarla convolución 1D ayuda bastante.\n",
        "\n",
        "Sin embargo, actualmente Keras provee una herramienta más potente para vectorizar texto, y se llama exactamente así, `TextVectorizer`. Vamos a ver cómo se simplifica el código anterior."
      ],
      "metadata": {
        "id": "kS-TGdoRzEgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el apartado 2 hemos trabajado con los datos pre-cargados en memoria (en la variable `texts`). Recuerda que `texts` era una lista de strings, que fue tokenizada a enteros en la variable `data`, y que tanto `data` como `labels` fueron desordenados. \n",
        "\n",
        "Vamos a ver otra forma de cargar este dataset en concreto. Para ello vamos a crear un objeto Dataset con la función [text_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory), que funciona de forma similar a como hacíamos con imágenes. Para ello, necesitamos que los datos estén estructurados por carpetas, una por cada subset (train, val, text), subcarpetas por etiqueta, y fichero de texto por cada ejemplo."
      ],
      "metadata": {
        "id": "FGosNztN1w_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"El contenido de aclImdb es: \")\n",
        "!ls aclImdb/\n",
        "print(\"El contenido de la carpeta train es: \")\n",
        "!ls aclImdb/train\n",
        "print(\"El contenido de la carpeta test es: \")\n",
        "!ls aclImdb/test"
      ],
      "metadata": {
        "id": "0QCGVzkm3oDw",
        "outputId": "90c25a9c-1975-41e5-a0a6-5ef0c7b32a7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El contenido de aclImdb es: \n",
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n",
            "El contenido de la carpeta train es: \n",
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n",
            "El contenido de la carpeta test es: \n",
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que la carpeta train tiene una subcarpeta unsup (que no es ni pos ni neg, las dos etiquetas que nos atañen). Por tanto, vamos a borrar dicha carpeta para que tanto train como test solo tengan las dos subcarpetas neg y pos."
      ],
      "metadata": {
        "id": "lQYAABp24AHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "9qicUnUO4VO5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación vamos a cargar los ejemplos con la función que hemos comentado antes. Esta vez vamos a tener un split de 20% validación y 80% train, ya que usaremos todo el conjunto de entrenamiento."
      ],
      "metadata": {
        "id": "foeRzizo4YNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Número de batches en raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Número de batches en raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Número de batches en raw_test_ds: {raw_test_ds.cardinality()}\")"
      ],
      "metadata": {
        "id": "2RtZqxoQ4fkw",
        "outputId": "35fcd82a-d1bd-4058-8969-c432e020e5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Número de batches en raw_train_ds: 625\n",
            "Número de batches en raw_val_ds: 157\n",
            "Número de batches en raw_test_ds: 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para ver un ejemplo, tenemos que acceder al dataset con un batch\n",
        "ejemplo=''\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  ejemplo=text_batch.numpy()[1]  # a numpy porque text_batch es un tensor\n",
        "\n",
        "print (ejemplo)"
      ],
      "metadata": {
        "id": "KEktzvcZ0drC",
        "outputId": "0e6d3417-81b6-4019-cca7-0ded1f5cd51e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"I consider this film one of the worst in the Nightmare series. It was so boring that I couldn't remember a thing 20 minutes after the film was over, it even tires me to write a review on it.<br /><br />Okay, #4 was a joke and Freddy was the joker. #5 tried to return to the roots of the series. It was darker and more atmospheric than Nightmare 4, which is a good thing, basically. They tried to shoot a horror film instead of a comedy. Unfortunately they forgot to add suspense and scares. Because of that Nightmare 5: The Dream Child is neither funny nor is it scary. What we actually get is a boring film with the usual bad actors (maybe with the exception of Lisa Wilcox).<br /><br />The plot (Freddy killing Lisa's friends by using the dreams of Lisa's unborn child) has a good base but it just isn't enough for 90 minutes of film. Sometimes the story gets very confusing (maybe because there isn't any) and you can't stop wondering what the filmmakers were aiming at. The screenplay must have had more holes than Swiss Cheese and the film therefore was very cheesy itself (let me say that I don't like cheese though, even if I am from Switzerland). Not even the special effects were as good as for example in part 4.<br /><br />Don't bother to rent/buy this film if not for completeness, it's quite a mess.<br /><br />My rating: 4/10 (get used to it, #6 is also a messy one...)\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si ejecutas varias veces la celda anterior, verás que en los ejemplos se nos cuela muchas veces la etiqueta HTML `<br />`. Antes no nos hizo falta porque al tokenizar el texto a tan solo 10000 palabras, esta etiqueta se eliminó. Ahora, sí sería bueno quitarlo. Primero, nos definimos una función que estandarice nuestro texto."
      ],
      "metadata": {
        "id": "aXAE_6c_6sFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data) #a minúscula\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")  # quita <br /> y lo reemplaza por un espacio.\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\" # quita signos de puntuación\n",
        "    )\n",
        "\n",
        "# aplicado al ejemplo anterior\n",
        "custom_standardization(ejemplo)\n"
      ],
      "metadata": {
        "id": "LwR7vj6667nT",
        "outputId": "0b703d05-10ed-4f1c-b2a7-aeeb65192163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'i consider this film one of the worst in the nightmare series it was so boring that i couldnt remember a thing 20 minutes after the film was over it even tires me to write a review on it  okay 4 was a joke and freddy was the joker 5 tried to return to the roots of the series it was darker and more atmospheric than nightmare 4 which is a good thing basically they tried to shoot a horror film instead of a comedy unfortunately they forgot to add suspense and scares because of that nightmare 5 the dream child is neither funny nor is it scary what we actually get is a boring film with the usual bad actors maybe with the exception of lisa wilcox  the plot freddy killing lisas friends by using the dreams of lisas unborn child has a good base but it just isnt enough for 90 minutes of film sometimes the story gets very confusing maybe because there isnt any and you cant stop wondering what the filmmakers were aiming at the screenplay must have had more holes than swiss cheese and the film therefore was very cheesy itself let me say that i dont like cheese though even if i am from switzerland not even the special effects were as good as for example in part 4  dont bother to rentbuy this film if not for completeness its quite a mess  my rating 410 get used to it 6 is also a messy one'>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, creamos una capa nueva que será la primera de un modelo. Esta capa es de tipo `TextVectorization`, y le indicamos la función para limpiar el texto definida arriba (por defecto también hace limpieza de signos de puntuación, pero nosotros queríamos quitar también las `<br />`), el número máximo de tokens (palabras) a tener en cuenta (sigamos con 10000, como antes), y el tamaño máximo de secuencia (como antes, vamos a poner 100). LA salida serán enteros (one-hot)."
      ],
      "metadata": {
        "id": "bvNRM53p8PTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import TextVectorization\n",
        "\n",
        "# Creamos la capa de tipo TextVectorization. Indicamos el máximo número de palabras y la longitud máxima de secuencia\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_words,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen,\n",
        ")\n",
        "\n",
        "# Hagamos un dataset de solo texto (sin etiquetas)\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Llamemos al método adapt, que creará el vocabulario (mapeo de las 10000 palabras a un entero)\n",
        "vectorize_layer.adapt(text_ds)\n"
      ],
      "metadata": {
        "id": "qzhDJ1uIsc42"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez hemos creado la capa de vectorización, y hemos calculado el vocabulario (palabra - número) con el método `adapt`, podemos añadirla al modelo. Primero metemos una capa de entrada de tipo string, y luego la de vectorización. El resto de modelo será igual que antes. Ahora sí, veremos que el entrenamiento es más lento que antes, por dos razones: el conjunto de train es mucho más grande, y el proceso de tokenización se hace sobre la marcha. Dado que usamos más ejemplos, veremos una mejora en la precisión del modelo."
      ],
      "metadata": {
        "id": "fpjkvbE39snx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelConv1D_3 = Sequential()\n",
        "# Necesitamos meter una capa de entrada indicando que recibirá texto\n",
        "modelConv1D_3.add(keras.Input(shape=(1,), dtype=tf.string, name='text'))\n",
        "# A continuación, esta capa pasará por la capa de vectorización\n",
        "modelConv1D_3.add(vectorize_layer)\n",
        "# El resto de capas queda igual que en el primer ejemplo de convolución 1D\n",
        "modelConv1D_3.add(layers.Embedding(max_words, \n",
        "                                   embedding_dim, \n",
        "                                   input_length=maxlen,\n",
        "                                   embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                                   trainable=False))\n",
        "modelConv1D_3.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "modelConv1D_3.add(layers.MaxPooling1D(5))\n",
        "modelConv1D_3.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "modelConv1D_3.add(layers.GlobalMaxPooling1D())\n",
        "modelConv1D_3.add(layers.Dense(1))\n",
        "\n",
        "modelConv1D_3.summary()\n",
        "\n",
        "modelConv1D_3.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "                    loss='binary_crossentropy',\n",
        "                   metrics=['acc'])\n",
        "history = modelConv1D_3.fit(raw_train_ds, \n",
        "                          epochs=10,\n",
        "                          batch_size=32,\n",
        "                         validation_data=(raw_val_ds))"
      ],
      "metadata": {
        "id": "V3qxi5Nf97TK",
        "outputId": "b8812531-8d08-4aaa-84bd-760858786f35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 100)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 94, 32)            22432     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 18, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 12, 32)            7200      \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Glo  (None, 32)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,029,665\n",
            "Trainable params: 29,665\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 39s 61ms/step - loss: 1.2461 - acc: 0.4994 - val_loss: 0.7384 - val_acc: 0.5092\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 20s 31ms/step - loss: 0.7164 - acc: 0.5397 - val_loss: 0.7330 - val_acc: 0.5284\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.6793 - acc: 0.5761 - val_loss: 0.7315 - val_acc: 0.5358\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.6523 - acc: 0.6108 - val_loss: 0.7493 - val_acc: 0.5386\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6296 - acc: 0.6374 - val_loss: 0.7507 - val_acc: 0.5492\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.6098 - acc: 0.6607 - val_loss: 0.7735 - val_acc: 0.5478\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.5924 - acc: 0.6758 - val_loss: 0.7470 - val_acc: 0.5580\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.5739 - acc: 0.6924 - val_loss: 0.7964 - val_acc: 0.5580\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 0.5572 - acc: 0.7103 - val_loss: 0.7782 - val_acc: 0.5720\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.5413 - acc: 0.7218 - val_loss: 0.8105 - val_acc: 0.5730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, lo podemos evaluar sobre el subconjunto de test. No necesitamos preprocesar el texto ya que la capa de vectorización hace todo el proceso automáticamente."
      ],
      "metadata": {
        "id": "SDc1FVAHApA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelConv1D_3.evaluate(raw_test_ds)"
      ],
      "metadata": {
        "id": "BxNagvY7AlDp",
        "outputId": "7068b2cb-7f01-4005-ec29-0a568ecfb9aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 47s 59ms/step - loss: 0.7707 - acc: 0.5725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7706998586654663, 0.5725200176239014]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ]
}